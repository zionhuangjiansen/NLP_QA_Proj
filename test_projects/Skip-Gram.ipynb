{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4828c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d9951d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2257673130.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    text = f.read())\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# 导入文件\n",
    "with open(file_path) as f:\n",
    "    text = f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae14383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess(text, freq):\n",
    "    text = text.lower()\n",
    "    text.text.replace(\".\", \"<PERIO>\")\n",
    "    #... \n",
    "    words = text.split()\n",
    "    word_counts = Counter(words) #[]\n",
    "    trimmed_words = [word for word in words if word_counts[word]>freq]\n",
    "    return trimmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ed3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备工作: 词典，文本转为数，训练样本准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeda35c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (380812899.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_words = [w for win int_words if random.random()<(1-prob_drop[w])]\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "words = preprocess(text)\n",
    "vocab = set(words)\n",
    "vocab2int = {w: c for c, w in enumerate(list(vocab))}\n",
    "int2vocab = {c: w for c, w in enumerate(list(vocab))}\n",
    "\n",
    "int_words = [vocab2int[w] for w in words]\n",
    "int_word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "word_freqs = {w: c/total_count for w,c in int_word_counts.items()}\n",
    "prob_drop = {w: 1-np.sqrt(t/word_freqs[w]) for w in int_word_counts}\n",
    "train_words = [w for win int_words if random.random()<(1-prob_drop[w])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db18c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取周边词/target\n",
    "def get_target(words, idx, window_size):\n",
    "    target_window = np.random.randint(1, window_size+1)\n",
    "    start_int = idx-target_window if (idx-target_window) > 0 else 0\n",
    "    end_point = idx+target_window\n",
    "    targets = set(words[start_point:idx]+words[idx+1:endpoint+1])\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de44c1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3778667883.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    for in in range(len(batch)):\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# batch 迭代器\n",
    "def get_batch(words, batch_size, window_size):\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        batch_x, batch_y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for in in range(len(batch)):\n",
    "            x = batch[i]\n",
    "            y = get_target(batch, i, window_size)\n",
    "            batch_x.extend([x]*len(y))\n",
    "            batch_y.extend(y)\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81568ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造网络结构\n",
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noice_dist = noise_dist\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # Initialize embedding tables with uniform distribution\n",
    "        # I believe this helps with convergence\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noice(self, batch_size, n_samples):\n",
    "        if self.noise_dist is None:\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist, batch_size*n_samples, replacement=True)\n",
    "        noise_vectors = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)\n",
    "\n",
    "        return noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4442c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造损失函数\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        #output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)\n",
    "        \n",
    "        # degate and sum correct and noiy log-sigoic loss\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "242f94c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3005137583.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    model = SkipGramNeg(len(vocab2int), embedding_dim)m\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "embedding_dim = 300\n",
    "model = SkipGramNeg(len(vocab2int), embedding_dim)m\n",
    "noise_dist = noise_dist\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "batch_size = 500\n",
    "n_samples = 5\n",
    "\n",
    "# train for some number of epochs \n",
    "for e in range(epochs):\n",
    "    # get our input, target batches\n",
    "    for input_words, target_words in get_batch(train_words, batch_size):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        # input, output and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(batch_size, n_samples)\n",
    "        \n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "        if steps//print_every = 0:\n",
    "            print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac66e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
